## 存在的问题

* 服务之间存在资源竞争，比如UDE/ES与spark作业的cpu、内存竞争，导致ES请求或者任务阻塞。
* 服务之间资源无法弹性共享，如ES、UDE占用的内存无法在空闲时给批处理作业(长服务占用的资源长期不释放)。
* 服务之间只有资源硬划分隔离方式。
* 服务内部需要自己实现资源管理和调度。
* 服务没有统一调度，有些服务跑在yarn上，有些服务跑在local上。
* 服务资源隔离没有统一，有些服务是jvm隔离，只能隔离内存，有些服务yarn隔离，可以隔离内存和cpu。

## 解决方案

针对现有的问题，我们希望达成以下的目标：

* 透明的系统资源访问： 对应用来说只需看到当前有多少资源可用， 去申请即可。
* 实现多业务之间的数据共享、 资源隔离、 资源弹性共享。
* 真正做到了批处理作业和长服务混合部署动态调度（ 应用管理）。


那么我们就提出了一套解决方案，可以描述为：
* 引入一套资源管理框架，可以承载全部计算类服务。
* 增加统一的应用管理层管理全部的服务。
* 将平台资源管理分为两层结构：第一层服务级别交给应用管理和资源管理框架，第二层app内部资源管理交给app自己。

这里的应用管理框架可以分为三个步骤：

* 通过预估模型判断任务需要的资源。
* 通过资源监视模块获取组件剩余资源。
* 通过上述资源的比较，调度作业。

### 预估模型

预估模型的主要作用就是估计一个任务需要的资源，可以理解为是一个先验模型。建立预估模型，当然，这个预估模型是一个经验模型，所以训练好这个模型后，需要更多的数据对模型进行改进。

关于估计模型的建立可以预先统计一些信息，比如可以是这样的模型：

* 每个表数据量(Row count)
* 每个表数据量：时间跨度
* 每个表数据量：时间跨度：检索语句，这里检索语句主要针对以下join、limit操作等。

在这个模型上，需要预估的资源就包括：

* cpu
* IO
* network
* memory

这里比较难预估的是内存，其实就是对每条数据的每个字段需要占用多大内存作预估，这个可以先粗略估计下每条数据占用多少内存，然后等作业做完后计算真实使用的内存，再调整先前的预估系数，相当于一个先验和后验的步骤。

针对预估模型不准确的问题，有很多种解决方案：
* 任务提交上来后，发现资源不够，那么可以先把任务放到等待队列中，然后定时定次数重新去获取spark和es的资源。
* 任务提交上来后，发现资源不够，那么可以检测正在运行的任务，获取消耗的资源量，这样判断是否是因为没有触发内存GC的缘故，导致了资源不够，这样的方案就可以监控下JVM进程的GC。

### 资源监视

通过预估模型就能大致知道用户的应用程序需要的资源，包括cpu，内存，磁盘等。然后通过资源监视模块可以知道每个组件剩余的可用资源。

#### spark的资源获取

通过如下方式就可以获取到每个spark应用的所有executor消耗的资源。

* http://localhost:4040/api/v1/applications/[app-id]/executors

具体例子比如这样访问：http://10.65.223.244:4040/api/v1/applications/app-20170525093745-0001/executors

可以获取到的数据有：
* id: 标识是executor还是driver。
* hostPort
* isActive
* rddBlocks
* memoryUsed
* diskUsed
* totalCores
* maxTasks
* activeTasks
* failedTasks
* completedTasks
* totalTasks
* totalDuration
* totalGCTime
* totalInputBytes
* totalShuffleRead
* totalShuffleWrite
* maxMemory
* executorLogs

#### es的资源获取

es有自带的api可以查看节点的资源使用情况。
* GET /_nodes：可以获取到服务器可用的内存、cpu、磁盘空间等。关键性的指标有：
  * heap_init_in_bytes
  * heap_max_in_bytes
  * non_heap_init_in_bytes
  * non_heap_max_in_bytes
  * direct_max_in_bytes
* GET /_nodes/stats?all=true：可以获取到存储在节点上的文档数，也提供了JVM、网络和文件系统的相关度量。关键性指标有：
  * docs：文档总数和删除文档数。
  * heap_used_in_bytes
  * heap_used_percent
  * heap_committed_in_bytes
  * heap_max_in_bytes
  * non_heap_used_in_bytes
  * non_heap_committed_in_bytes

es有很多插件也可以看到资源使用情况，比如：Elasticsearch-head、Bigdesk、Marvel、Kopf、Kibana、Nagios等。

### 任务调度

根据预估模型和资源使用情况，会对任务作调度，可以把任务分为三种状态：运行，拒绝，等待。
* 如果资源够用那么就运行任务。
* 如果任务需要的资源超过了最大资源，那么就拒绝该任务。
* 如果任务需要的资源没有超过最大资源，但是超过了可用资源，那么就把任务放入等待队列，当资源够用时在调度任务。

这里如果任务被提交上去，那么就需要监控

## 具体实现

资源调度模块总体是独立的，并且是可配置可插拔式的。

可配置的参数包括：
* 预估任务资源时，每个任务占内存和cpu的系数。
* 预估系统资源时，系统资源的安全系数。
* 预估内存资源时，每条数据占用的内存比例。

任务调度是可插拔式的，也就是说兼容spark的任务调度模块，我们自己实现的任务调度模块也需要兼容spark，目前spark兼容的任务调度模块有：

* yarn
* mesos
* standalone
* local

在spark的任务调度中，直接依赖于SchedulerBackend，SchedulerBackend与实际资源管理模块交互实现资源请求。在这里，CoarseGrainedSchedulerBackend继承了ScheduleBackend，是Spark中与资源调度相关的最重要的抽象，抽象出与TaskScheduler通信的逻辑，同时还要能够与各种不同的第三方资源管理系统无缝地交互。StandaloneSchedulerBackend，MesosCoarseGrainedSchedulerBackend，YarnSchedulerBackend又继承了CoarseGrainedSchedulerBackend，所以我们自己实现的资源调度模块也是需要继承CoarseGrainedSchedulerBackend，这样就实现了自己的资源管理。
